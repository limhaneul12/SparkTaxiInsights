{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "MAX_MEMORY = \"5g\"\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"TripAnaliysis\")\n",
    "    .config(\"spark.excutor.memory\", MAX_MEMORY)\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "class FileLoadInParquet:\n",
    "    def __init__(self, year: int, taxi_type: str) -> None:\n",
    "        self.year = year\n",
    "        self.taxi_type = taxi_type\n",
    "    \n",
    "    def location(self) -> list[str]:\n",
    "        return str(Path(os.getcwd()).parent.joinpath(f\"data/{self.taxi_type}\"))\n",
    "\n",
    "    def read_parquet_data(self, spark: SparkSession) -> DataFrame:\n",
    "        # 파케이 파일 경로\n",
    "        data: str = self.location()\n",
    "        return spark.read.parquet(f\"file:///{data}/*\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-02.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-03.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-04.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-05.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-06.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-07.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-08.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-09.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-10.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-11.parquet'),\n",
    " PosixPath('/Users/imhaneul/Documents/spark-kafka-distribute/sparkAnaliysis/data/2019/fhvhv_tripdata_2019-12.parquet')]\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = FileLoadInParquet(2019, \"YellowTaxi\").read_parquet_data(spark)\n",
    "# data = spark.read.parquet(f\"file:///{data}/*\")\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_groupby(data: DataFrame, name: str, agg_name: str) -> DataFrame:\n",
    "    return (\n",
    "        data.select(F.split(col(name), \" \")[0].name(\"pickup\"))\n",
    "        .dropna()\n",
    "        .groupBy(\"pickup\")\n",
    "        .agg(F.count(\"*\").name(agg_name))\n",
    "    )\n",
    "\n",
    "def datetime_miles_average(data: DataFrame) -> DataFrame:\n",
    "    return (data.select(F.split(col(\"pickup_datetime\"), \" \")[0].name(\"pickup\"), col(\"trip_miles\"))\n",
    "        .groupBy(\"pickup\") \n",
    "        .agg(\n",
    "            F.count(\"pickup\").name(\"pickup_total\"), \n",
    "            F.avg(\"trip_miles\").name(\"average_miles\")\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "request_groupby = datetime_groupby(data, \"request_datetime\", \"request_count\")\n",
    "trip_groupby = datetime_groupby(data, \"pickup_datetime\", \"trip_count\")\n",
    "drop_groupby = datetime_groupby(data, \"dropoff_datetime\", \"drop_count\")\n",
    "average_mile = datetime_miles_average(data)\n",
    "\n",
    "rtd_join = (\n",
    "    trip_groupby\n",
    "    .join(request_groupby, on=\"pickup\", how=\"left\")\n",
    "    .join(drop_groupby, on=\"pickup\", how=\"left\")\n",
    "    .join(average_mile, on=\"pickup\", how=\"left\")\n",
    ").orderBy(\"pickup\")\n",
    "\n",
    "week_day_rtd_join = (\n",
    "    rtd_join.select(\n",
    "        F.date_format(col(\"pickup\"), \"EEEE\").alias(\"week\"),\n",
    "        F.dayofweek(col(\"pickup\")).alias(\"week_number\"),\n",
    "        col(\"pickup\"), \n",
    "        col(\"trip_count\"), \n",
    "        col(\"request_count\"), \n",
    "        col(\"drop_count\"),\n",
    "        col(\"average_miles\")\n",
    "    )\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "week_day_rtd_join.to_csv(\"test.csv\", index=False)\n",
    "group_date_average_mlies = week_day_rtd_join.groupby(\"week\").average_miles.mean().to_frame().reset_index()\n",
    "group_date_average_mlies[\"sort_dow\"] = group_date_average_mlies[\"week\"].replace({\n",
    "    \"Sunday\": 0,\n",
    "    \"Monday\": 1,\n",
    "    \"Tuesday\": 2,\n",
    "    \"Wednesday\": 3,\n",
    "    \"Thursday\": 4,\n",
    "    \"Friday\": 5,\n",
    "    \"Saturday\": 6,  \n",
    "})\n",
    "group_date_average_mlies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_date_trip = week_day_rtd_join.groupby(\"week\").trip_count.mean().to_frame().reset_index()\n",
    "group_date_trip[\"sort_dow\"] = group_date_trip[\"week\"].replace({\n",
    "    \"Sunday\": 0,\n",
    "    \"Monday\": 1,\n",
    "    \"Tuesday\": 2,\n",
    "    \"Wednesday\": 3,\n",
    "    \"Thursday\": 4,\n",
    "    \"Friday\": 5,\n",
    "    \"Saturday\": 6,  \n",
    "})\n",
    "group_date_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10), constrained_layout=True)\n",
    "\n",
    "# Plot 2: Weekly average miles\n",
    "sns.lineplot(x=\"week\", y=\"average_miles\", data=group_date_average_mlies.sort_values(\"sort_dow\"), ax=ax[0], palette='husl', legend=False)\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "ax[0].set_xlabel(\"Week\")\n",
    "ax[0].set_ylabel(\"Average Miles\")\n",
    "ax[0].set_title(\"Weekly Average Miles (2019-02)\")\n",
    "\n",
    "# Plot 2: Weekly average miles\n",
    "sns.barplot(x=\"week\", y=\"trip_count\", data=group_date_trip.sort_values(\"sort_dow\"), ax=ax[1], palette='husl', legend=False)\n",
    "ax[1].tick_params(axis='x', rotation=45)\n",
    "ax[1].set_xlabel(\"Week\")\n",
    "ax[1].set_ylabel(\"Average Miles\")\n",
    "ax[1].set_title(\"Weekly Average Miles (2019-02)\")\n",
    "# Overall Title\n",
    "plt.suptitle(\"Analysis of NYC Taxi Data (February 2019)\", fontsize=16)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
