{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/10 18:03:51 WARN Utils: Your hostname, limhaneul-ui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.109 instead (on interface en0)\n",
      "24/05/10 18:03:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/10 18:03:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "MAX_MEMORY = \"8g\"\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"TripAnaliysis\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.excutor.memory\", MAX_MEMORY)\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "    .config(\"spark.sql.caseSensitive\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "class FileLoadInParquet:\n",
    "    def __init__(self, year: int, taxi_type: str) -> None:\n",
    "        self.year = year\n",
    "        self.taxi_type = taxi_type\n",
    "    \n",
    "    def location(self) -> list[str]:\n",
    "        return str(Path(os.getcwd()).parent.joinpath(f\"data/{self.taxi_type}/{self.year}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, IntegerType\n",
    "\n",
    "# 주어진 스키마에 따라 PySpark의 데이터 타입으로 변환\n",
    "schema = StructType([\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"passenger_count\", LongType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"rate_code\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", LongType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2009 ~ 2024년 별로 스키마가 다름 정규화 작업 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------+---------------+------------------+------------------+---------+---------+-----------------+------------------+---------+------------+-----------------+---------+-------+-------+---------+---------+---------+\n",
      "|vendor_name|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|Passenger_Count|     Trip_Distance|         Start_Lon|Start_Lat|Rate_Code|store_and_forward|           End_Lon|  End_Lat|Payment_Type|         Fare_Amt|surcharge|mta_tax|Tip_Amt|Tolls_Amt|Total_Amt|Rate_code|\n",
      "+-----------+--------------------+---------------------+---------------+------------------+------------------+---------+---------+-----------------+------------------+---------+------------+-----------------+---------+-------+-------+---------+---------+---------+\n",
      "|        VTS| 2009-01-04 02:52:00|  2009-01-04 03:02:00|              1|              2.63|        -73.991957|40.721567|     null|             null|        -73.993803|40.695922|        CASH|              8.9|      0.5|   null|    0.0|      0.0|      9.4|      nan|\n",
      "|        VTS| 2009-01-04 03:31:00|  2009-01-04 03:38:00|              3|              4.55|        -73.982102| 40.73629|     null|             null|         -73.95585| 40.76803|      Credit|             12.1|      0.5|   null|    2.0|      0.0|     14.6|      nan|\n",
      "|        VTS| 2009-01-03 15:43:00|  2009-01-03 15:57:00|              5|             10.35|        -74.002587|40.739748|     null|             null|        -73.869983|40.770225|      Credit|             23.7|      0.0|   null|   4.74|      0.0|    28.44|      nan|\n",
      "|        DDS| 2009-01-01 20:52:58|  2009-01-01 21:14:00|              1|               5.0|        -73.974267|40.790955|     null|             null|-73.99655799999998|40.731849|      CREDIT|             14.9|      0.5|   null|   3.05|      0.0|    18.45|      nan|\n",
      "|        DDS| 2009-01-24 16:18:23|  2009-01-24 16:24:56|              1|               0.4|         -74.00158|40.719382|     null|             null|-74.00837799999998| 40.72035|        CASH|              3.7|      0.0|   null|    0.0|      0.0|      3.7|      nan|\n",
      "|        DDS| 2009-01-16 22:35:59|  2009-01-16 22:43:35|              2|               1.2|        -73.989806|40.735006|     null|             null|        -73.985021|40.724494|        CASH|              6.1|      0.5|   null|    0.0|      0.0|      6.6|      nan|\n",
      "|        DDS| 2009-01-21 08:55:57|  2009-01-21 09:05:42|              1|               0.4|-73.98404999999998|40.743544|     null|             null|         -73.98026|40.748926|      CREDIT|              5.7|      0.0|   null|    1.0|      0.0|      6.7|      nan|\n",
      "|        VTS| 2009-01-04 04:31:00|  2009-01-04 04:36:00|              1|              1.72|        -73.992635|40.748362|     null|             null|        -73.995585|40.728307|        CASH|              6.1|      0.5|   null|    0.0|      0.0|      6.6|      nan|\n",
      "|        CMT| 2009-01-05 16:29:02|  2009-01-05 16:40:21|              1|               1.6|         -73.96969|40.749244|     null|             null|        -73.990413|40.751082|      Credit|8.699999999999998|      0.0|   null|    1.3|      0.0|     10.0|      nan|\n",
      "|        CMT| 2009-01-05 18:53:13|  2009-01-05 18:57:45|              1|0.6999999999999998|        -73.955173|40.783044|     null|             null|-73.95859799999998|40.774822|        Cash|              5.9|      0.0|   null|    0.0|      0.0|      5.9|      nan|\n",
      "+-----------+--------------------+---------------------+---------------+------------------+------------------+---------+---------+-----------------+------------------+---------+------------+-----------------+---------+-------+-------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+-------------------+-------------------+---------------+------------------+------------------+---------------+---------+------------------+------------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|     trip_distance|  pickup_longitude|pickup_latitude|rate_code|store_and_fwd_flag| dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+------------------+------------------+---------------+---------+------------------+------------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|      VTS|2010-01-26 07:41:00|2010-01-26 07:45:00|              1|              0.75|        -73.956778|       40.76775|        1|              null|        -73.965957|       40.765232|         CAS|        4.5|      0.0|    0.5|       0.0|         0.0|         5.0|\n",
      "|      DDS|2010-01-30 23:31:00|2010-01-30 23:46:12|              1|               5.9|-73.99611799999998|      40.763932|        1|              null|-73.98151199999998|       40.741193|         CAS|       15.3|      0.5|    0.5|       0.0|         0.0|        16.3|\n",
      "|      DDS|2010-01-18 20:22:20|2010-01-18 20:38:12|              1|               4.0|        -73.979673|       40.78379|        1|              null|-73.91785199999998|        40.87856|         CAS|       11.7|      0.5|    0.5|       0.0|         0.0|        12.7|\n",
      "|      VTS|2010-01-09 01:18:00|2010-01-09 01:35:00|              2|               4.7|        -73.977922|      40.763997|        1|              null|-73.92390799999998|       40.759725|         CAS|       13.3|      0.5|    0.5|       0.0|         0.0|        14.3|\n",
      "|      CMT|2010-01-18 19:10:14|2010-01-18 19:17:07|              1|0.5999999999999999|        -73.990924|      40.734682|        1|                 0|-73.99551099999998|       40.739088|         Cre|        5.3|      0.0|    0.5|      0.87|         0.0|        6.67|\n",
      "|      DDS|2010-01-23 18:40:25|2010-01-23 18:54:51|              1|               3.3|               0.0|            0.0|        1|              null|               0.0|             0.0|         CRE|       10.5|      0.0|    0.5|       1.0|         0.0|        12.0|\n",
      "|      VTS|2010-01-17 09:18:00|2010-01-17 09:25:00|              1|              1.33|        -73.993747|      40.754917|        1|              null|-73.98471499999998|       40.755927|         CAS|        6.1|      0.0|    0.5|       0.0|         0.0|         6.6|\n",
      "|      VTS|2010-01-09 13:49:00|2010-01-09 13:56:00|              1|              1.83|         -73.97103|      40.751307|        1|              null|         -73.99056|       40.734923|         CAS|        6.9|      0.0|    0.5|       0.0|         0.0|         7.4|\n",
      "|      VTS|2010-01-09 00:25:00|2010-01-09 00:39:00|              1|              3.28|        -73.990037|      40.725633|        1|              null|        -73.993842|       40.761697|         CAS|       11.3|      0.5|    0.5|       0.0|         0.0|        12.3|\n",
      "|      VTS|2010-01-27 18:15:00|2010-01-27 18:29:00|              1|              1.42|        -73.979623|       40.74378|        1|              null|-73.98941499999998|       40.756788|         Cre|        8.5|      1.0|    0.5|       2.0|         0.0|        12.0|\n",
      "+---------+-------------------+-------------------+---------------+------------------+------------------+---------------+---------+------------------+------------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2011-01-01 09:10:00|  2011-01-01 09:12:00|              4|          0.0|         1|              null|         145|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|\n",
      "|       2| 2011-01-01 09:04:00|  2011-01-01 09:13:00|              4|          0.0|         1|              null|         264|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|\n",
      "|       2| 2011-01-01 09:14:00|  2011-01-01 09:16:00|              4|          0.0|         1|              null|         264|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|\n",
      "|       2| 2011-01-01 09:04:00|  2011-01-01 09:06:00|              5|          0.0|         1|              null|         146|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|\n",
      "|       2| 2011-01-01 09:08:00|  2011-01-01 09:08:00|              5|          0.0|         1|              null|         146|         146|           1|        2.5|  0.5|    0.5|      0.11|         0.0|                  0.0|        3.61|                null|       null|\n",
      "|       2| 2011-01-01 09:23:00|  2011-01-01 09:23:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       2| 2011-01-01 09:25:00|  2011-01-01 09:25:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       1| 2011-01-01 09:58:10|  2011-01-01 10:15:35|              1|          8.0|         1|                 N|         138|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|\n",
      "|       1| 2011-01-01 09:23:27|  2011-01-01 09:39:39|              1|          1.6|         1|                 N|         170|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|\n",
      "|       1| 2011-01-01 09:42:08|  2011-01-01 09:51:50|              4|          2.5|         1|                 N|         237|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2012-01-01 09:07:56|  2012-01-01 09:12:09|              1|          0.9|         1|                 N|         158|         231|           2|        4.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.9|                null|       null|\n",
      "|       1| 2012-01-01 09:18:49|  2012-01-01 09:30:01|              1|          2.3|         1|                 N|         231|         164|           2|        8.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.5|                null|       null|\n",
      "|       1| 2012-01-01 09:31:38|  2012-01-01 09:46:05|              1|          2.2|         1|                 N|         164|         148|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|\n",
      "|       1| 2012-01-01 09:47:35|  2012-01-01 09:55:57|              4|          0.9|         1|                 N|         148|         107|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|\n",
      "|       1| 2012-01-01 09:57:08|  2012-01-01 10:02:42|              3|          0.7|         1|                 N|         107|         107|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.5|                null|       null|\n",
      "|       1| 2012-01-01 09:47:10|  2012-01-01 10:06:18|              1|          4.7|         1|                 N|         226|         239|           1|       14.9|  0.5|    0.5|       2.0|         0.0|                  0.0|        17.9|                null|       null|\n",
      "|       1| 2012-01-01 09:16:42|  2012-01-01 09:40:14|              1|          2.3|         1|                 N|         234|         237|           2|       12.9|  0.5|    0.5|       0.0|         0.0|                  0.0|        13.9|                null|       null|\n",
      "|       1| 2012-01-01 09:40:46|  2012-01-01 10:03:49|              1|          2.2|         1|                 N|         237|          68|           1|       12.9|  0.5|    0.5|       2.0|         0.0|                  0.0|        15.9|                null|       null|\n",
      "|       1| 2012-01-01 09:10:04|  2012-01-01 09:20:54|              1|          2.4|         1|                 N|         161|         263|           1|        8.9|  0.5|    0.5|      2.97|         0.0|                  0.0|       12.87|                null|       null|\n",
      "|       1| 2012-01-01 09:28:10|  2012-01-01 09:32:50|              1|          1.4|         1|                 N|         237|         236|           1|        5.3|  0.5|    0.5|      1.89|         0.0|                  0.0|        8.19|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2013-01-01 09:39:00|  2013-01-01 09:55:00|              3|         3.86|         1|              null|         238|         116|           2|       15.0|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.0|                null|       null|\n",
      "|       2| 2013-01-01 09:12:00|  2013-01-01 09:16:00|              5|          0.0|         1|              null|         264|         264|           1|        3.5|  0.5|    0.5|      0.12|         0.0|                  0.0|        4.62|                null|       null|\n",
      "|       2| 2013-01-01 09:02:00|  2013-01-01 09:03:00|              3|          0.0|         1|              null|         264|         264|           1|        2.5|  0.5|    0.5|      0.25|         0.0|                  0.0|        3.75|                null|       null|\n",
      "|       2| 2013-01-01 09:38:00|  2013-01-01 09:38:00|              2|          0.0|         1|              null|         264|         264|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       2| 2013-01-01 09:03:00|  2013-01-01 09:04:00|              4|          0.0|         1|              null|         264|         264|           1|        3.0|  0.5|    0.5|      0.07|         0.0|                  0.0|        4.07|                null|       null|\n",
      "|       2| 2013-01-01 09:03:00|  2013-01-01 09:04:00|              3|          0.0|         1|              null|         146|         146|           1|        2.5|  0.5|    0.5|      0.25|         0.0|                  0.0|        3.75|                null|       null|\n",
      "|       2| 2013-01-01 09:05:00|  2013-01-01 09:09:00|              4|          0.0|         1|              null|         146|         146|           1|        4.5|  0.5|    0.5|      1.25|         0.0|                  0.0|        6.75|                null|       null|\n",
      "|       2| 2013-01-01 09:06:00|  2013-01-01 09:09:00|              4|          0.0|         1|              null|         146|         146|           1|        4.0|  0.5|    0.5|      0.15|         0.0|                  0.0|        5.15|                null|       null|\n",
      "|       2| 2013-01-01 09:14:00|  2013-01-01 09:16:00|              3|          0.0|         1|              null|         146|         146|           1|        3.0|  0.5|    0.5|      0.05|         0.0|                  0.0|        4.05|                null|       null|\n",
      "|       2| 2013-01-01 09:04:00|  2013-01-01 09:06:00|              5|          0.0|         1|              null|         130|         130|           1|        3.0|  0.5|    0.5|      0.45|         0.0|                  0.0|        4.45|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2014-01-01 09:02:00|  2014-01-01 09:04:00|              6|          0.0|         1|              null|         146|         146|           1|        3.5|  0.5|    0.5|      0.02|         0.0|                  0.0|        4.52|                null|       null|\n",
      "|       2| 2014-01-01 09:06:00|  2014-01-01 09:09:00|              5|          0.0|         1|              null|         146|         146|           1|        3.5|  0.5|    0.5|      0.05|         0.0|                  0.0|        4.55|                null|       null|\n",
      "|       2| 2014-01-01 09:10:00|  2014-01-01 09:13:00|              5|          0.0|         1|              null|         146|         146|           1|        3.5|  0.5|    0.5|      0.08|         0.0|                  0.0|        4.58|                null|       null|\n",
      "|       2| 2014-01-01 09:54:00|  2014-01-01 09:55:00|              5|          0.0|         1|              null|         264|         264|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       1| 2014-01-01 09:29:18|  2014-01-01 09:35:13|              2|          1.8|         1|                 N|         229|         262|           2|        7.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         8.5|                null|       null|\n",
      "|       1| 2014-01-01 09:36:33|  2014-01-01 09:55:19|              3|          4.8|         1|                 N|         140|           7|           2|       17.5|  0.5|    0.5|       0.0|         0.0|                  0.0|        18.5|                null|       null|\n",
      "|       1| 2014-01-01 09:20:43|  2014-01-01 09:31:07|              2|          2.0|         1|                 N|          82|         226|           2|        9.5|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.5|                null|       null|\n",
      "|       1| 2014-01-01 09:48:52|  2014-01-01 09:56:14|              1|          1.1|         1|                 N|         162|         170|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|\n",
      "|       1| 2014-01-01 09:45:29|  2014-01-01 09:55:33|              1|          1.6|         1|                 N|          90|          79|           1|        8.5|  0.5|    0.5|       1.0|         0.0|                  0.0|        10.5|                null|       null|\n",
      "|       1| 2014-01-01 09:57:12|  2014-01-01 10:05:39|              3|          1.3|         1|                 N|          79|          90|           1|        7.0|  0.5|    0.5|       2.0|         0.0|                  0.0|        10.0|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2015-01-01 09:11:33|  2015-01-01 09:16:48|              1|          1.0|         1|                 N|          41|         166|           1|        5.7|  0.5|    0.5|       1.4|         0.0|                  0.0|         8.4|                null|       null|\n",
      "|       1| 2015-01-01 09:18:24|  2015-01-01 09:24:20|              1|          0.9|         1|                 N|         166|         238|           3|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.3|                null|       null|\n",
      "|       1| 2015-01-01 09:26:19|  2015-01-01 09:41:06|              1|          3.5|         1|                 N|         238|         162|           1|       13.2|  0.5|    0.5|       2.9|         0.0|                  0.0|        17.4|                null|       null|\n",
      "|       1| 2015-01-01 09:45:26|  2015-01-01 09:53:20|              1|          2.1|         1|                 N|         162|         263|           1|        8.2|  0.5|    0.5|      2.37|         0.0|                  0.0|       11.87|                null|       null|\n",
      "|       1| 2015-01-01 09:59:21|  2015-01-01 10:05:24|              1|          1.0|         1|                 N|         236|         141|           3|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.3|                null|       null|\n",
      "|       1| 2015-01-01 09:07:31|  2015-01-01 09:11:32|              1|          0.8|         1|                 N|         239|         238|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|\n",
      "|       1| 2015-01-01 09:47:08|  2015-01-01 09:54:50|              1|          1.1|         1|                 N|         238|         239|           2|        7.0|  0.5|    0.5|       0.0|         0.0|                  0.0|         8.3|                null|       null|\n",
      "|       1| 2015-01-01 09:58:04|  2015-01-01 10:11:56|              1|          2.9|         1|                 N|         238|          42|           1|       12.2|  0.5|    0.5|       2.7|         0.0|                  0.0|        16.2|                null|       null|\n",
      "|       1| 2015-01-01 09:29:25|  2015-01-01 09:37:25|              2|          1.3|         1|                 N|          90|         125|           2|        7.0|  0.5|    0.5|       0.0|         0.0|                  0.0|         8.3|                null|       null|\n",
      "|       1| 2015-01-01 09:39:02|  2015-01-01 10:02:37|              2|          4.3|         1|                 N|         125|         141|           2|       18.0|  0.5|    0.5|       0.0|         0.0|                  0.0|        19.3|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2016-01-01 09:12:22|  2016-01-01 09:29:14|              1|          3.2|         1|                 N|          48|         262|           1|       14.0|  0.5|    0.5|      3.06|         0.0|                  0.3|       18.36|                null|       null|\n",
      "|       1| 2016-01-01 09:41:31|  2016-01-01 09:55:10|              2|          1.0|         1|                 N|         162|          48|           2|        9.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.8|                null|       null|\n",
      "|       1| 2016-01-01 09:53:37|  2016-01-01 09:59:57|              1|          0.9|         1|                 N|         246|          90|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|                null|       null|\n",
      "|       1| 2016-01-01 09:13:28|  2016-01-01 09:18:07|              1|          0.8|         1|                 N|         170|         162|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                null|       null|\n",
      "|       1| 2016-01-01 09:33:04|  2016-01-01 09:47:14|              1|          1.8|         1|                 N|         161|         140|           2|       11.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                null|       null|\n",
      "|       1| 2016-01-01 09:49:47|  2016-01-01 10:04:44|              1|          2.3|         1|                 N|         141|         137|           1|       11.0|  0.5|    0.5|      2.45|         0.0|                  0.3|       14.75|                null|       null|\n",
      "|       1| 2016-01-01 09:41:58|  2016-01-01 10:22:06|              1|         13.8|         1|                 N|         100|          53|           2|       43.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        44.3|                null|       null|\n",
      "|       2| 2016-01-01 09:25:28|  2016-01-01 09:55:46|              5|         3.46|         1|                 N|          48|          79|           2|       20.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        21.3|                null|       null|\n",
      "|       2| 2016-01-01 09:56:57|  2016-01-01 10:02:24|              4|         0.83|         1|                 N|          79|         107|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|                null|       null|\n",
      "|       2| 2016-01-01 09:10:08|  2016-01-01 09:23:05|              1|         0.87|         1|                 N|         164|         164|           2|        7.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2017-01-01 09:32:05|  2017-01-01 09:37:48|              1|          1.2|         1|                 N|         140|         236|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                null|       null|\n",
      "|       1| 2017-01-01 09:43:25|  2017-01-01 09:47:42|              2|          0.7|         1|                 N|         237|         140|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                null|       null|\n",
      "|       1| 2017-01-01 09:49:10|  2017-01-01 09:53:53|              2|          0.8|         1|                 N|         140|         237|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|                null|       null|\n",
      "|       1| 2017-01-01 09:36:42|  2017-01-01 09:41:09|              1|          1.1|         1|                 N|          41|          42|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|                null|       null|\n",
      "|       1| 2017-01-01 09:07:41|  2017-01-01 09:18:16|              1|          3.0|         1|                 N|          48|         263|           2|       11.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                null|       null|\n",
      "|       1| 2017-01-01 09:20:52|  2017-01-01 09:24:59|              2|          0.7|         1|                 N|         236|         262|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                null|       null|\n",
      "|       1| 2017-01-01 09:33:49|  2017-01-01 09:42:38|              2|          1.6|         1|                 N|         236|         238|           1|        8.0|  0.5|    0.5|      1.85|         0.0|                  0.3|       11.15|                null|       null|\n",
      "|       1| 2017-01-01 09:48:22|  2017-01-01 09:52:15|              2|          0.6|         1|                 N|         238|         239|           1|        5.0|  0.5|    0.5|      1.25|         0.0|                  0.3|        7.55|                null|       null|\n",
      "|       1| 2017-01-01 09:57:12|  2017-01-01 10:06:28|              2|          1.0|         1|                 N|         239|          48|           1|        7.5|  0.5|    0.5|      1.75|         0.0|                  0.3|       10.55|                null|       null|\n",
      "|       1| 2017-01-01 09:10:25|  2017-01-01 09:29:06|              1|          1.0|         1|                 N|         246|          48|           2|       12.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        13.3|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "24/05/10 18:06:48 ERROR Executor: Exception in task 0.0 in stage 39.0 (TID 39)\n",
      "org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n",
      "\t... 20 more\n",
      "24/05/10 18:06:48 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 39) (192.168.0.109 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n",
      "\t... 20 more\n",
      "\n",
      "24/05/10 18:06:48 ERROR TaskSetManager: Task 0 in stage 39.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o117.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 39) (192.168.0.109 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb 셀 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m null_checking\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m year \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2009\u001b[39m, \u001b[39m2025\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# data = read_parquet_data(year, spark).printSchema()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# print(data)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     a \u001b[39m=\u001b[39m read_parquet_data(year, spark)\u001b[39m.\u001b[39;49mshow(\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/jupyter_trips/spark_null_checking.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# checking_count(read_parquet_data(year, spark)).toPandas().to_csv(f\"{year}_null_checking.csv\", index=False, index_label=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-kafka-distribute-2w72GDIg-py3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-kafka-distribute-2w72GDIg-py3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-kafka-distribute-2w72GDIg-py3.11/lib/python3.11/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-kafka-distribute-2w72GDIg-py3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o117.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 39) (192.168.0.109 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/imhaneul/Documents/project/SparkTaxiInsights/sparkAnaliysis/data/YellowTaxi/2018/yellow_tripdata_2018-03.parquet. Column: [congestion_surcharge], Expected: double, Found: INT32\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "\n",
    "def read_parquet_data(year: int, spark: SparkSession) -> DataFrame:\n",
    "    # 파케이 파일 경로\n",
    "    data: str = FileLoadInParquet(year, \"YellowTaxi\").location()\n",
    "    return (\n",
    "        spark.read\n",
    "        .parquet(f\"file:///{data}/*\"))\n",
    "\n",
    "def checking_count(data: DataFrame) -> DataFrame:\n",
    "    sum_null: list[Column] = [F.sum(col(c).isNull().cast(IntegerType())).alias(c) for c in data.columns]\n",
    "    null_checking: None = data.agg(*sum_null).cache()\n",
    "\n",
    "    return null_checking\n",
    "\n",
    "for year in range(2009, 2025):\n",
    "    # data = read_parquet_data(year, spark).printSchema()\n",
    "    # print(data)\n",
    "    a = read_parquet_data(year, spark).show(10)\n",
    "    # checking_count(read_parquet_data(year, spark)).toPandas().to_csv(f\"{year}_null_checking.csv\", index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def month_classification(name: str) -> DataFrame:\n",
    "    return F.split(col(name), \" \", )[0].alias(\"datetime\")\n",
    "    \n",
    "\n",
    "col_name = \"Trip_Pickup_DateTime\"\n",
    "# Replace 'data' with your DataFrame\n",
    "null_check = data.withColumn(\n",
    "    \"year\",\n",
    "    F.when(col(col_name) != \"null\",\n",
    "           F.year(month_classification(col_name)).cast(IntegerType())).otherwise(None)\n",
    ").withColumn(\n",
    "    \"month\",\n",
    "    F.when(col(col_name) != \"null\",\n",
    "           F.month(month_classification(col_name)).cast(IntegerType())).otherwise(None)\n",
    ").withColumn(\n",
    "    \"day\",\n",
    "    F.when(col(col_name) != \"null\",\n",
    "           F.dayofmonth(month_classification(col_name)).cast(IntegerType())).otherwise(None)\n",
    ").select(\n",
    "    col(\"Trip_Pickup_DateTime\"),\n",
    "    col(\"year\"),\n",
    "    col(\"month\"),\n",
    "    col(\"day\"),\n",
    ").groupBy(col(\"month\"), col(\"day\")).agg(F.count(\"*\").alias(\"count\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_delect = null_check.filter(col(\"month\").isNotNull())\n",
    "filtered = null_check.select(\n",
    "    \"*\"\n",
    ").orderBy(\n",
    "    F.asc(col(\"month\")), \n",
    "    F.asc(col(\"day\")),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-kafka-distribute-2w72GDIg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
